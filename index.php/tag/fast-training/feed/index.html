<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>fast training &#8211; PlainSwipe</title>
	<atom:link href="http://localhost/index.php/tag/fast-training/feed/" rel="self" type="application/rss+xml" />
	<link>http://localhost:8000</link>
	<description></description>
	<lastBuildDate>Sun, 01 Jan 2023 07:41:35 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.2</generator>

<image>
	<url>http://localhost:8000/wp-content/uploads/2023/04/cropped-logo-32x32.png</url>
	<title>fast training &#8211; PlainSwipe</title>
	<link>http://localhost:8000</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>Practical Ways to speed up training a PyTorch model</title>
		<link>http://localhost:8000/index.php/practical-wats-to-speed-up-training-a-pytorch-model/</link>
					<comments>http://localhost:8000/index.php/practical-wats-to-speed-up-training-a-pytorch-model/#respond</comments>
		
		<dc:creator><![CDATA[rohitkumar1979]]></dc:creator>
		<pubDate>Sun, 01 Jan 2023 07:41:35 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[fast training]]></category>
		<category><![CDATA[learning rate decay]]></category>
		<category><![CDATA[learning rate scheduler]]></category>
		<guid isPermaLink="false">https://plainswipe.com/?p=965</guid>

					<description><![CDATA[Optimize the learning rate: Choosing an appropriate learning rate can significantly impact training speed and model performance. You can use techniques such as learning rate decay or the 1cycle learning rate schedule to find an optimal learning rate. Learning Rate Decay Learning rate decay involves reducing the learning rate over time as the model trains.&#8230;&#160;<a href="http://localhost:8000/index.php/practical-wats-to-speed-up-training-a-pytorch-model/" class="" rel="bookmark">Read More &#187;<span class="screen-reader-text">Practical Ways to speed up training a PyTorch model</span></a>]]></description>
										<content:encoded><![CDATA[
<p>Optimize the learning rate: Choosing an appropriate learning rate can significantly impact training speed and model performance. You can use techniques such as learning rate decay or the 1cycle learning rate schedule to find an optimal learning rate.</p>



<h4 class="wp-block-heading">Learning Rate Decay</h4>



<p>Learning rate decay involves reducing the learning rate over time as the model trains. This can help the model converge to a minimum in the loss function and improve model performance. Here is an example of how to implement learning rate decay using PyTorch&#8217;s optimizers:</p>



<pre class="wp-block-preformatted">import torch.optim as optim<br /><br /># Initialize optimizer with a learning rate of 0.1<br />optimizer = optim.SGD(model.parameters(), lr=0.1)<br /><br /># Set the learning rate decay factor and decay step size<br />decay_factor = 0.1<br />decay_step_size = 10<br /><br /># Define the learning rate scheduling<br />scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=decay_step_size, gamma=decay_factor)<br /><br /># Train the model<br />for epoch in range(num_epochs):<br />  # Decay the learning rate at each epoch<br />  scheduler.step()<br />  # Train the model on a batch of data<br />  ...<br /></pre>



<p>The 1cycle learning rate schedule involves increasing the learning rate from a low value to a high value, then decreasing it back to the low value over the course of training. This schedule can help the model escape from local minima in the loss function and improve model performance. Here is an example of how to implement the 1cycle learning rate schedule using PyTorch&#8217;s optimizers:</p>



<pre class="wp-block-preformatted">import torch.optim as optim

# Initialize the optimizer with a low learning rate
optimizer = optim.SGD(model.parameters(), lr=1e-5)

# Set the maximum learning rate and the number of iterations in each phase of the 1cycle schedule
max_lr = 0.1
num_iterations = 1000

# Define the 1cycle learning rate schedule
scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, steps_per_epoch=num_iterations)

# Train the model
for epoch in range(num_epochs):
  # Update the learning rate at each iteration
  scheduler.step()
  # Train the model on a batch of data
  ...
</pre>



<h4 class="wp-block-heading">Tips of learning rate schedules</h4>



<p>Use a learning rate finder to help choose an appropriate learning rate. A learning rate finder involves training the model with a range of learning rates and plotting the resulting loss values. The learning rate at the point of divergence can be used as a starting point for further training.</p>



<p>Monitor the loss and accuracy of the model as it trains to ensure that the learning rate schedule is effective. If the loss is not decreasing or the accuracy is not improving, you may need to adjust the learning rate schedule.</p>



<p>Don&#8217;t use a learning rate that is too high or too low. A learning rate that is too high may cause the model to diverge, while a learning rate that is too low may cause the model to converge too slowly.to implement a learning rate finder</p>



<p>Here is an example of how to implement a learning rate finder using PyTorch&#8217;s optimizers:</p>



<pre class="wp-block-preformatted">import matplotlib.pyplot as plt<br />import torch<br />import torch.optim as optim<br /><br /># Define the model, loss function, and optimizer<br />model = ...<br />loss_fn = ...<br />optimizer = optim.SGD(model.parameters(), lr=1e-7)<br /><br /># Set the learning rate range and the number of iterations<br />min_lr = 1e-10<br />max_lr = 1.0<br />num_iterations = 100<br /><br /># Create a learning rate scheduler<br />scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)<br /><br /># Define a list to store the learning rates and losses<br />learning_rates = []<br />losses = []<br /><br /># Train the model with different learning rates<br />for iteration in range(num_iterations):<br />  # Update the learning rate<br />  learning_rate = min_lr * (max_lr / min_lr) ** (iteration / num_iterations)<br />  optimizer.param_groups[0]['lr'] = learning_rate<br />  scheduler.step()<br />  learning_rates.append(learning_rate)<br /><br />  # Train the model on a batch of data<br />  model.train()<br />  inputs, labels = ...<br />  optimizer.zero_grad()<br />  outputs = model(inputs)<br />  loss = loss_fn(outputs, labels)<br />  loss.backward()<br />  optimizer.step()<br />  losses.append(loss.item())<br /><br /># Plot the learning rates and losses<br />plt.plot(learning_rates, losses)<br />plt.xscale('log')<br />plt.xlabel('Learning rate')<br />plt.ylabel('Loss')<br />plt.show()<br /></pre>
]]></content:encoded>
					
					<wfw:commentRss>http://localhost:8000/index.php/practical-wats-to-speed-up-training-a-pytorch-model/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Practical guide for training or fine tuning large language models</title>
		<link>http://localhost:8000/index.php/practical-guide-for-training-or-fine-tuning-large-language-models/</link>
					<comments>http://localhost:8000/index.php/practical-guide-for-training-or-fine-tuning-large-language-models/#respond</comments>
		
		<dc:creator><![CDATA[rohitkumar1979]]></dc:creator>
		<pubDate>Thu, 08 Dec 2022 19:34:00 +0000</pubDate>
				<category><![CDATA[Deep Learning]]></category>
		<category><![CDATA[distributed training]]></category>
		<category><![CDATA[fast training]]></category>
		<category><![CDATA[peer to peer]]></category>
		<guid isPermaLink="false">https://plainswipe.com/?p=1068</guid>

					<description><![CDATA[Imagine if you can connect to a friends computer and use that to speed up training of your model. Training ML models is very expensive, and ML techniques require many iterations hence they require speed. The Solution: Distributed Computing Distributed training is a technique that allows you to train machine learning models using multiple computers.&#8230;&#160;<a href="http://localhost:8000/index.php/practical-guide-for-training-or-fine-tuning-large-language-models/" class="" rel="bookmark">Read More &#187;<span class="screen-reader-text">Practical guide for training or fine tuning large language models</span></a>]]></description>
										<content:encoded><![CDATA[
<p>Imagine if you can connect to a friends computer and use that to speed up training of your model.</p>



<p>Training ML models is very expensive, and ML techniques require many iterations hence they require speed.</p>



<h4 class="wp-block-heading">The Solution: Distributed Computing </h4>



<p>Distributed training is a technique that allows you to train machine learning models using multiple computers. This can be useful when you have large datasets or models that are too computationally intensive to train on a single computer.</p>



<p>Changing is setting up the infrastructure to enable multiple computers to work together. One way to do this is to use a Common Internet File System (CIFS) to share data and resources between the computers.</p>



<p>To set up a CIFS, you can use a single script that is capable of running on multiple operating systems (OS). This script can be used to install the necessary software and configure the CIFS on each computer. For example, on a Linux computer, the script can install the CIFS utilities and configure the Samba server, while on a MacOS or Windows computer, the script can install the SMBFS or SAMBA command-line utilities and mount the shared directory.</p>



<p>Once the CIFS is set up, you can start the model training process on each computer. By starting the same training process on different computers, you can connect the various training processes and allow them to communicate with each other.</p>



<p>The various training processes communicate with each other using a process called message passing. This involves sending messages between the computers to exchange information about the model training, such as the gradients of the model parameters. The computers also discover each other using a process called process discovery. This involves identifying the other computers that are part of the distributed training process and determining how to communicate with them.</p>



<p>In summary, distributed training allows you to train machine learning models using multiple computers. By setting up a CIFS and starting the same training process on different computers, you can connect the various training processes and enable them to communicate and discover each other. This can be useful when you have large datasets or models that are too computationally intensive to train on a single computer.</p>



<h4 class="wp-block-heading">How to start, stop training on the various computers?</h4>



<p>To start and stop model training on the computers connected to a Common Internet File System (CIFS) using PyTorch, you can use the following techniques:</p>



<ol>
<li>Start training on all computers simultaneously: You can start training on all computers simultaneously by running the training script on each computer at the same time. This can be done manually by starting the script on each computer, or automatically using a tool like <code>fabric</code> or <code>ansible</code>.</li>



<li>Start training on one computer and then add more computers: You can start training on one computer and then add more computers to the training process as needed. To do this, you can use the <code>torch.nn.DataParallel</code> wrapper to parallelize the model training across the computers. You can then add more computers to the training process by adding them to the <code>DataParallel</code> wrapper. For example:</li>
</ol>



<p></p>



<pre class="wp-block-preformatted">import torch<br /><br /># Set the data directory to the mount point<br />data_dir = "/path/to/mount/point"<br /><br /># Load the dataset<br />dataset = torch.utils.data.DatasetFolder(data_dir, ...)<br /><br /># Set up the model and DataParallel wrapper with one computer<br />model = MyModel()<br />model = torch.nn.DataParallel(model, device_ids=[0])<br /><br /># Use DataParallel to parallelize the model training<br />for input, target in dataset:<br />    output = model(input)<br />    loss = loss_fn(output, target)<br />    loss.backward()<br />    optimizer.step()<br /><br /># Add more computers to the DataParallel wrapper<br />model = torch.nn.DataParallel(model, device_ids=[0, 1, 2])<br /></pre>



<p>This will allow you to start training on one computer and then add more computers to the training process as needed.</p>



<ol start="3">
<li>Stop training on one or more computers: To stop training on one or more computers, you can simply stop the training script on those computers.</li>
</ol>



<h2 class="wp-block-heading">Code that demonstrates and tests the CLIFS set up</h2>



<p>To demonstrate and test the Common Internet File System (CIFS) set up, you can write a Python script that performs the following tasks:</p>



<ol>
<li>Create a test file on one of the computers.</li>



<li>Check if the test file is accessible from the other computers.</li>



<li>Modify the test file on one of the computers.</li>



<li>Check if the modifications are reflected on the other computers.</li>
</ol>



<p>Here is an example of a Python script that demonstrates and tests the CIFS set up:</p>



<pre class="wp-block-preformatted">import os<br /><br /># Create a test file on one of the computers<br />with open("Z:/test.txt", "w") as f:<br />    f.write("This is a test file.")<br /><br /># Check if the test file is accessible from the other computers<br />if os.path.exists("/path/to/mount/point/test.txt"):<br />    print("Test file is accessible from the other computers.")<br />else:<br />    print("Test file is not accessible from the other computers.")<br /><br /># Modify the test file on one of the computers<br />with open("Z:/test.txt", "a") as f:<br />    f.write("\nThis line was added later.")<br /><br /># Check if the modifications are reflected on the other computers<br />with open("/path/to/mount/point/test.txt", "r") as f:<br />    content = f.read()<br />    if "This line was added later." in content:<br />        print("Modifications are reflected on the other computers.")<br />    else:<br />        print("Modifications are not reflected on the other computers<br /></pre>



<h4 class="wp-block-heading">Setting up CIFS</h4>



<p>Here is a Python script that you can use to detect the operating system, IP address, and set up a Common Internet File System (CIFS) on each computer:</p>



<pre class="wp-block-preformatted">import platform<br />import os<br /><br /># Detect the operating system<br />os_name = platform.system()<br /><br /># Get the IP address<br />ip_address = os.popen('hostname -I').read()<br />ip_address = ip_address.strip()<br /><br /># Set up the CIFS file system<br />if os_name == "Linux":<br />    # Install the CIFS utilities<br />    os.system("sudo apt-get install cifs-utils")<br /><br />    # Create a shared directory<br />    os.system("mkdir /path/to/shared/directory")<br /><br />    # Edit the Samba configuration file<br />    with open("/etc/samba/smb.conf", "a") as f:<br />        f.write("\n[shared]\n")<br />        f.write("path = /path/to/shared/directory\n")<br />        f.write("writable = yes\n")<br />        f.write("guest ok = yes\n")<br /><br />    # Restart the Samba server<br />    os.system("sudo systemctl restart smbd")<br /><br />elif os_name == "Darwin":<br />    # Install the SMBFS or SAMBA command-line utilities<br />    os.system("brew install smbfs")<br />    # or<br />    os.system("brew install samba")<br /><br />    # Create a mount point<br />    os.system("mkdir /path/to/mount/point")<br /><br />    # Mount the shared directory<br />    os.system("sudo mount_smbfs //guest@{}/shared /path/to/mount/point".format(ip_address))<br /><br />elif os_name == "Windows":<br />    # Mount the shared directory using the "net use" command<br />    os.system("net use Z: \\\\{}\\shared".format(ip_address))<br /><br />else:<br />    print("Unsupported operating system.")<br /><br />print("CIFS file system set up complete.")<br /></pre>



<h2 class="wp-block-heading">Tying it up all together</h2>



<p>To set up a distributed training environment on multiple computers using a main computer and the username and password provided by the computer owners, you can use a tool that automates the installation and configuration of software on remote servers. One such tool is <code>ansible</code>, which is a configuration management tool that allows you to define the desired state of your infrastructure and then automatically enforce that state.</p>



<p>Here is an example of how you can use <code>ansible</code> to set up a distributed training environment on multiple computers, including the Common Internet File System (CIFS), PyTorch, training script, dataset, etc., and then execute the training:</p>



<ol>
<li>Write an <code>ansible</code> playbook that specifies the required steps to set up the training environment. For example:</li>
</ol>



<pre class="wp-block-preformatted">---<br />- name: Set up distributed training environment<br />  hosts: all<br />  tasks:<br />    - name: Install Python<br />      package:<br />        name: python3<br />      become: yes<br /><br />    - name: Install PyTorch<br />      pip:<br />        name: torch<br />      become: yes<br /><br />    - name: Install CIFS utilities<br />      package:<br />        name: cifs-utils<br />      become: yes<br /><br />    - name: Mount shared directory<br />      mount:<br />        path: /path/to/mount/point<br />        src: //10.0.0.1/shared<br />        fstype: cifs<br />        opts: username=guest,password=password<br />      become: yes<br /><br />    - name: Install training code<br />      copy:<br />        src: training.py<br />        dest: /opt/training/training.py<br />      become: yes<br /><br />    - name: Install dataset<br />      copy:<br />        src: dataset.zip<br />        dest: /opt/training/dataset.zip<br />      become: yes<br />      unarchive:<br />        src: /opt/training/dataset.zip<br />        dest: /opt/training/<br />        remote_src: true<br /><br />    - name: Execute training<br />      command: python3 /opt/training/training.py<br />      become:<br />---<br />- name: Set up distributed training environment<br />  hosts: all<br />  tasks:<br />    - name: Install Python<br />      package:<br />        name: python3<br />      become: yes<br /><br />    - name: Install PyTorch<br />      pip:<br />        name: torch<br />      become: yes<br /><br />    - name: Install CIFS utilities<br />      package:<br />        name: cifs-utils<br />      become: yes<br /><br />    - name: Mount shared directory<br />      mount:<br />        path: /path/to/mount/point<br />        src: //10.0.0.1/shared<br />        fstype: cifs<br />        opts: username=guest,password=password<br />      become: yes<br /><br />    - name: Install training code<br />      copy:<br />        src: training.py<br />        dest: /opt/training/training.py<br />      become: yes<br /><br />    - name: Install dataset<br />      copy:<br />        src: dataset.zip<br />        dest: /opt/training/dataset.zip<br />      become: yes<br />      unarchive:<br />        src: /opt/training/dataset.zip<br />        dest: /opt/training/<br />        remote_src: true<br /><br />    - name: Execute training<br />      command: python3 /opt/training/training.py<br />      become: yes<br /></pre>



<p>Create an <code>ansible</code> inventory file that specifies the connection information for the computers. For example:</p>



<pre class="wp-block-preformatted">[windows]<br />windows_computer ansible_user=username ansible_password=password<br /><br />[linux]<br />linux_computer ansible_user=username ansible_password=password<br /><br />[mac]<br />mac_computer ansible_user=username ansible_ssh_private_key_file=/path/to/keyfile<br /></pre>



<p>Use <code>ansible</code> to apply the playbook to the computers. For example:</p>



<pre class="wp-block-preformatted">ansible-playbook -i inventory.ini playbook.yml</pre>



<p>To allow a non-technical person to provide the information that <code>ansible</code> requires to connect to their computer, you can follow these steps:</p>



<ol>
<li>Explain to the person what <code>ansible</code> is and why you need access to their computer.</li>



<li>Provide the person with the <code>ansible</code> inventory file template that you will use to connect to their computer. This should include the hostname or IP address, username, and password or private key for the computer.</li>



<li>Ask the person to fill in the <code>ansible</code> inventory file template with the appropriate information for their computer.</li>



<li>To find the hostname or IP address of the computer, the person can do the following:</li>
</ol>



<ul>
<li>On Windows:
<ol>
<li>Press the <code>Windows</code> + <code>R</code> keys to open the <code>Run</code> dialog.</li>



<li>Type <code>cmd</code> and press <code>Enter</code> to open the command prompt.</li>



<li>Type <code>ipconfig</code> and press <code>Enter</code>.</li>



<li>Look for the <code>IPv4 Address</code> field. This is the IP address of the computer.</li>
</ol>
</li>



<li>On MacOS:
<ol>
<li>Click the <code>Apple</code> menu and select <code>System Preferences</code>.</li>



<li>Click the <code>Network</code> icon.</li>



<li>Select the active network connection (e.g., Wi-Fi) from the list on the left.</li>



<li>Click the <code>Advanced</code> button.</li>



<li>Click the <code>TCP/IP</code> tab.</li>



<li>The <code>IP address</code> field displays the IP address of the computer.</li>
</ol>
</li>



<li>On Linux:
<ol>
<li>Open a terminal.</li>



<li>Type <code>ifconfig</code> and press <code>Enter</code>.</li>



<li>Look for the <code>inet</code> field. This is the IP address of the computer.</li>
</ol>
</li>
</ul>



<ol start="5">
<li>To find the username and password of the computer, the person can do the following:</li>
</ol>



<ul>
<li>On Windows:
<ol>
<li>Press the <code>Windows</code> + <code>R</code> keys to open the <code>Run</code> dialog.</li>



<li>Type <code>control userpasswords2</code> and press <code>Enter</code> to open the <code>User Accounts</code> dialog.</li>



<li>Click the <code>Manage another account</code> link.</li>



<li>Click the account that you want to use to connect to the computer.</li>



<li>Click the <code>Change the account name</code> or <code>Change the password</code> button to view or change the username or password for the account.</li>
</ol>
</li>



<li>On MacOS:
<ol>
<li>Click the <code>Apple</code> menu and select <code>System Preferences</code>.</li>



<li>Click the <code>Users &amp; Groups</code> icon.</li>



<li>Click the <code>Lock</code> icon and enter the administrator password to make changes.</li>



<li>Click the user account that you want to use to connect to the computer.</li>



<li>Click the <code>Change Password</code> button to view or change the password for the account.</li>
</ol>
</li>



<li>On Linux:
<ol>
<li>Open a terminal.</li>



<li>Type <code>id</code> and press <code>Enter</code> to view the current user and</li>
</ol>
</li>
</ul>
]]></content:encoded>
					
					<wfw:commentRss>http://localhost:8000/index.php/practical-guide-for-training-or-fine-tuning-large-language-models/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
	</channel>
</rss>
