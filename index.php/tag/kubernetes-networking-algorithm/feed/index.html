<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Kubernetes networking algorithm &#8211; PlainSwipe</title>
	<atom:link href="http://localhost/index.php/tag/kubernetes-networking-algorithm/feed/" rel="self" type="application/rss+xml" />
	<link>http://localhost:8000</link>
	<description></description>
	<lastBuildDate>Sat, 07 Jan 2023 09:42:51 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.2</generator>

<image>
	<url>http://localhost:8000/wp-content/uploads/2023/04/cropped-logo-32x32.png</url>
	<title>Kubernetes networking algorithm &#8211; PlainSwipe</title>
	<link>http://localhost:8000</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>How to visualize features in a fine tuned LLM using PyTorch</title>
		<link>http://localhost:8000/index.php/how-to-visualize-features-in-a-fine-tuned-llm-using-pytorch/</link>
					<comments>http://localhost:8000/index.php/how-to-visualize-features-in-a-fine-tuned-llm-using-pytorch/#respond</comments>
		
		<dc:creator><![CDATA[rohitkumar1979]]></dc:creator>
		<pubDate>Sat, 07 Jan 2023 09:42:51 +0000</pubDate>
				<category><![CDATA[Cloud Computing]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<category><![CDATA[Kubernetes]]></category>
		<category><![CDATA[Kubernetes networking algorithm]]></category>
		<guid isPermaLink="false">https://plainswipe.com/?p=1084</guid>

					<description><![CDATA[To visualize the features of a fine-tuned language model in PyTorch, you can use a technique called &#8220;gradient-weighted class activation mapping&#8221; (Grad-CAM). This technique allows you to visualize which parts of the input text are most important for the model&#8217;s prediction. Here&#8217;s an example of how you can implement Grad-CAM in PyTorch: 1. First, you&#8217;ll&#8230;&#160;<a href="http://localhost:8000/index.php/how-to-visualize-features-in-a-fine-tuned-llm-using-pytorch/" class="" rel="bookmark">Read More &#187;<span class="screen-reader-text">How to visualize features in a fine tuned LLM using PyTorch</span></a>]]></description>
										<content:encoded><![CDATA[
<p>To visualize the features of a fine-tuned language model in PyTorch, you can use a technique called &#8220;gradient-weighted class activation mapping&#8221; (Grad-CAM). This technique allows you to visualize which parts of the input text are most important for the model&#8217;s prediction.</p>



<p>Here&#8217;s an example of how you can implement Grad-CAM in PyTorch:</p>



<p>1. First, you&#8217;ll need to choose a layer of the model whose activations you want to visualize. It&#8217;s often a good idea to choose a layer near the end of the model, since these layers tend to capture higher-level features of the input.</p>



<p>2. Next, you&#8217;ll need to compute the gradient of the output of the model with respect to the activations of the chosen layer. You can do this using PyTorch&#8217;s backward function.</p>



<p>3. Once you have the gradients, you can average them across the different channels of the activations to obtain a &#8220;gradient weight&#8221; for each position in the input text.</p>



<p>4. Finally, you can apply a heatmap visualization to the input text, using the gradient weights as the intensity values for the heatmap. This will show you which parts of the input text are most important for the model&#8217;s prediction.</p>



<h2 class="wp-block-heading">Code</h2>



<pre class="wp-block-preformatted">import torch<br>import torch.nn as nn<br><br># Assume that we have a fine-tuned language model called "model"<br># and a batch of input text called "inputs"<br><br># Choose a layer of the model to visualize<br>layer = model.bert.encoder.layer[11]<br><br># Set the model to evaluation mode<br>model.eval()<br><br># Forward pass<br>with torch.no_grad():<br>    outputs = model(inputs)<br><br># Choose a class to visualize<br>class_idx = 0<br><br># Compute the gradient of the output with respect to the activations of the chosen layer<br>outputs[0, class_idx].backward()<br><br># Obtain the gradients of the activations<br>gradients = layer.weight.grad<br><br># Average the gradients across the different channels<br>gradients = gradients.mean(dim=1).mean(dim=1)<br><br># Apply a heatmap visualization to the input text<br>heatmap = (gradients * inputs).sum(dim=1).squeeze()<br><br># The heatmap will have the same size as the input text, and the intensity values<br># will show which parts of the input text are most important for the model's prediction<br></pre>
]]></content:encoded>
					
					<wfw:commentRss>http://localhost:8000/index.php/how-to-visualize-features-in-a-fine-tuned-llm-using-pytorch/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
	</channel>
</rss>
